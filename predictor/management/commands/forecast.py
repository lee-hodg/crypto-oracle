import pandas as pd
import pickle
import logging
import tensorflow as tf
from django.core.management.base import BaseCommand
from predictor.models import TrainingSession, StockPrediction
from utils.utils import valid_date
from utils.utils import load_data, model_forecast, denormalize_forecast
from predictor.models import Stock


logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = '''
        - Load the dataset
        - Load model
        - Make forecasts

        run ./manage forecast.py
        '''

    def add_arguments(self, parser):
        parser.add_argument('-T', '--training-session-id', dest='training_session_id',
                            type=str, help='ID of a training session')
        # parser.add_argument('-U', '--update-forecast', dest='update_forecast', action='store_true',
        #                     help='Update the forecast with new stock data')
        # parser.add_argument('-S', '--start-date', dest='start_date', default='2018-01-01',
        #                     type=valid_date, help='Start date')
        # parser.add_argument('-E', '--end-date', dest='end_date',
        #                     default=Stock.objects.latest('dt').dt.strftime('%Y-%m-%d %H:%M:%S'),
        #                     type=valid_date, help='End date')

    def handle(self, *args, **options):

        training_session_id = options['training_session_id']

        # start_date = options.get('start_date')
        # end_date = options.get('end_date')

        # Load the Django model corresponding to these options
        try:
            training_session = TrainingSession.objects.get(id=training_session_id)
        except TrainingSession.DoesNotExist as dn_exc:
            logger.error(f'No training session with this id: {training_session_id}')
            return
        start_date = training_session.start_date
        end_date = training_session.end_date

        if training_session.stockprediction_set.count() > 0:
            logger.debug("Generate new stock predictions")
            # Make predictions from after the last we already have and until the end of the stock data stored
            start_date = training_session.stockprediction_set.latest('dt').dt
            end_date = None
            latest_stock_date = Stock.objects.latest('dt').dt
            if start_date >= latest_stock_date:
                logger.debug('No new stock data')
                return

        # Load the Keras model (note the weights_file path is autogenerated on save)
        model = tf.keras.models.load_model(training_session.weights_path)

        logger.debug(f'Forecast over {start_date} to {end_date}')
        # Load the data
        df = load_data(training_session, start_date=start_date, end_date=end_date, forecast=True)
        if len(df) == 0:
            logger.debug('No new data to update')
            return

        # Load the data scaler from disk
        scaler = pickle.load(open(training_session.scaler_path, 'rb'))
        date_range = df.iloc[training_session.window_length:].index
        dataset = scaler.transform(df['close'].values.reshape(-1, 1)).flatten()

        # Make the forecast
        window_len = training_session.window_length
        output_size = training_session.output_size
        if len(dataset) == 0:
            logger.debug('No dataset')
            return
        preds = model_forecast(model, dataset, window_len)

        # E.g if window_len is 5, we have predictions for [5:]  since [0, 1, 2, 3, 4] -> [5] etc. If the output_size=1
        # then we neglect the final pred since it uses the final 5 elements of training set to pred a subsequent
        # element, which
        # we have no training data to compare with
        res_df = pd.DataFrame({'dt': date_range,
                               'y': dataset.flatten()[window_len:], 'yhat': preds.flatten()[:-output_size]})

        # Want to inverse the normalization transform
        if scaler is not None:
            res_df['y_prices'] = scaler.inverse_transform(res_df['y'].values.reshape(-1, 1)).flatten()
            res_df['yhat_prices'] = scaler.inverse_transform(res_df['yhat'].values.reshape(-1, 1)).flatten()
        else:
            # Window scaling
            res_df['y_prices'] = res_df['y']
            res_df['yhat_prices'] = denormalize_forecast(res_df['yhat'], dataset)

        logger.debug(res_df.head())

        # Bulk insert the predictions in the db
        model_instances = [StockPrediction(training_session=training_session,
                                           dt=result.dt,
                                           prediction=float(result.yhat_prices),
                                           actual=float(result.y_prices)
                                           )
                           for result in res_df.itertuples()]
        StockPrediction.objects.bulk_create(model_instances)
