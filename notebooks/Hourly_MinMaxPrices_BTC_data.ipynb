{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20ij_Wn-hwGA"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This project is the capstone project of the [Udacity Datascience Nangodegree](https://www.udacity.com/course/data-scientist-nanodegree--nd025). It attempts to use ML techniques such as LSTMs to predict the prices of crypto currencies. The inputs to the algorithm will be trading data over a given data range and the prediction will be the adjusted close price.\n",
    "\n",
    "The accompanying web app will allow the selection of a given cyrpto currency from a dropdown, a given input data range and a given algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfv_5Cz2hwGP"
   },
   "source": [
    "# Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gnRP-XAhwGP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import scipy.stats as stats\n",
    "import plotly.offline as offline_py\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfR4-SEShwGQ",
    "outputId": "7d315a32-6c76-44d6-8912-78a34535f989"
   },
   "outputs": [],
   "source": [
    "# Check TF 2.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saMWfCBMhwGR"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSEPXJ2ChwGS"
   },
   "outputs": [],
   "source": [
    "color_scheme = {\n",
    "    'index': '#B6B2CF',\n",
    "    'etf': '#2D3ECF',\n",
    "    'tracking_error': '#6F91DE',\n",
    "    'df_header': 'silver',\n",
    "    'df_value': 'white',\n",
    "    'df_line': 'silver',\n",
    "    'heatmap_colorscale': [(0, '#6F91DE'), (0.5, 'grey'), (1, 'red')],\n",
    "    'background_label': '#9dbdd5',\n",
    "    'low_value': '#B6B2CF',\n",
    "    'high_value': '#2D3ECF',\n",
    "    'y_axis_2_text_color': 'grey',\n",
    "    'shadow': 'rgba(0, 0, 0, 0.75)',\n",
    "    'major_line': '#2D3ECF',\n",
    "    'minor_line': '#B6B2CF',\n",
    "    'main_line': 'black'}\n",
    "\n",
    "def generate_config():\n",
    "    return {'showLink': False, 'displayModeBar': False, 'showAxisRangeEntryBoxes': True}\n",
    "\n",
    "def _generate_traces(name_df_color_data):\n",
    "    traces = []\n",
    "\n",
    "    for name, df, color in name_df_color_data:\n",
    "        traces.append(go.Scatter(\n",
    "            name=name,\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode='lines',\n",
    "            line={'color': color}))\n",
    "\n",
    "    return traces\n",
    "\n",
    "def resample(open_prices, high_prices, low_prices, close_prices, period='H'):\n",
    "    \"\"\"Converts daily OHLC prices to OHLC prices in the period\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    open_prices : DataFrame\n",
    "        Daily open prices for each ticker and date\n",
    "    high_prices : DataFrame\n",
    "        Daily high prices for each ticker and date\n",
    "    low_prices : DataFrame\n",
    "        Daily low prices for each ticker and date\n",
    "    close_prices : DataFrame\n",
    "        Daily close prices for each ticker and date\n",
    "    period: the resample period e.g W, H, D etc\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    open_prices_weekly : DataFrame\n",
    "        Resampled open prices for each ticker and date\n",
    "    high_prices_weekly : DataFrame\n",
    "        Resampled high prices for each ticker and date\n",
    "    low_prices_weekly : DataFrame\n",
    "        Resampled low prices for each ticker and date\n",
    "    close_prices_weekly : DataFrame\n",
    "        Resampled close prices for each ticker and date\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    open_prices_weekly = open_prices.resample('H').first()\n",
    "    high_prices_weekly = high_prices.resample('H').max()\n",
    "    low_prices_weekly = low_prices.resample('H').min()\n",
    "    close_prices_weekly = close_prices.resample('H').last()\n",
    "    \n",
    "    return open_prices_weekly, high_prices_weekly, low_prices_weekly, close_prices_weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGN5umG5hwGT"
   },
   "source": [
    "# Bitcoin Price Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5eeOug2hwGT"
   },
   "source": [
    "To begin, let's take a look a historical bitcoin (btc) price data.\n",
    "\n",
    "We use yfinance to source the data, which includes the open, high, low, close (OHLC) data in addition to the adjusted close price and the volume traded that day.\n",
    "\n",
    "The open price is the price of the stock at the start of that trading data, the high is the highest price the stock attains over the course of the day, the low is likewise the lowest price over the day and the close is the price of the stock at the end of trading that day. \n",
    "\n",
    "The adjusted close is to take into account things like dividends being paid out.\n",
    "\n",
    "The volume is the number of shares traded over the day.Generally large volumes of buy orders would increase a stock's price and large volumes of sell orders would decrease its price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/bitstampUSD_1-min_data_2012-01-01_to_2020-12-31.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a datetime from the timestamp\n",
    "df['Datetime'] = df['Timestamp'].apply(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Timestamp', 'Open', 'High', 'Low', 'Volume_(BTC)', 'Volume_(Currency)', 'Weighted_Price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to just hourly\n",
    "hourly_df = df.resample('H').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many NaN do we have?\n",
    "print(f\"The percentage of NaN values is {100*hourly_df['Close'].isna().sum()/hourly_df.shape[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_df = hourly_df[hourly_df.index > '2018-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The percentage of recent NaN values is {100*recent_df['Close'].isna().sum()/recent_df.shape[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After 2018 not many missing hourly close prices....We could impute by just assuming the missing close \n",
    "recent_df[recent_df['Close'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df = recent_df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df[int_df['Close'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_df[(recent_df.index > '2020-04-25 15:00') & (recent_df.index < '2020-04-25 23:00') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df[(int_df.index > '2020-04-25 15:00') & (int_df.index < '2020-04-25 23:00') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(int_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_df[int_df.index > '2020-09-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV-5NWxXhwGk"
   },
   "source": [
    "## Some config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMPliSdOhwGk"
   },
   "source": [
    "Here are a bunch of LSTM helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xgs6SNXhwGk"
   },
   "outputs": [],
   "source": [
    "def split_data(data, training_size=0.8):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets\n",
    "    We want to preserve order and not shuffle at this stage as past points will be used to predict next in the sequence\n",
    "    The test set will represent the unseen future\n",
    "    However the windowed dataset will shuffle around each windows and along with target label (see that func)\n",
    "    \n",
    "    Params:\n",
    "        data: the dataset\n",
    "        training_sie: the split e.g. 0.8 means 80% of data is used in the training set\n",
    "    \"\"\"\n",
    "    return data[:int(training_size*len(data))], data[int(training_size*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYmc5Y5_hwGk"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset(series, shuffle_buffer, window_len, batch_size, window_scaling=False):\n",
    "    \"\"\"\n",
    "    If we have a series like [1,2,3,4,5,6]\n",
    "    We want to split it into windows, e.g. we take previous value of the series\n",
    "    as the X input features and want to predict the following value as output Y\n",
    "    e.g. [1,2] - > 3, so what we want to do is split the data into windows\n",
    "    of length window_len + 1 (the +1 acconts for the label)\n",
    "    E.g.\n",
    "      [1, 2, 3]\n",
    "      [2, 3, 4]\n",
    "      [3, 4, 5]\n",
    "      [4, 5, 6]\n",
    "\n",
    "\n",
    "    We shuffle the data to avoid bias\n",
    "\n",
    "    Finally we split the example into input/target\n",
    "\n",
    "    [[1, 2], [3]]\n",
    "    .\n",
    "    .\n",
    "\n",
    "    so it's appropriate to feed into the model.fit    \n",
    "\n",
    "    and we batch it into batches of batch_size\n",
    "    \n",
    "    Params:\n",
    "        series: the series upon which we perform the windowing\n",
    "        shuffle_buffer: size of the buffer when shuffling\n",
    "            (see https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#shuffle)\n",
    "        window_len; how many previous elements to take into account when predicting the next\n",
    "        batch_size: https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#batch\n",
    "    \"\"\"\n",
    "    # Initially the data is (1188,) expand dims to TensorShape([1188, 1])\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "    # will be an iterable of tf.Tensor([998.325], shape=(1,), dtype=float32),...\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/55429307/how-to-use-windows-created-by-the-dataset-window-method-in-tensorflow-2-0\n",
    "    # The +1 accounts for the label too. Create a bunch of windows over our series\n",
    "    # If we started with ds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5])\n",
    "    # then ds = ds.window(3, shift=1, drop_remainder=False) would lead\n",
    "    # to [1,2,3], [2, 3, 4], [3, 4, 5], [4, 5], [5] whereas \n",
    "    # drop_remainder=True) => [1,2,3], [2, 3, 4], [3, 4, 5]\n",
    "    # Remember the first window_len are our training data and the 1 is \n",
    "    # the target/label\n",
    "    # Could also do this with pandas shift\n",
    "    ds = ds.window(window_len + 1, shift=1, drop_remainder=True)\n",
    "    #for w in ds:\n",
    "    #    print(list(w.as_numpy_iterator()))\n",
    "    \n",
    "    # Maps map_func across this dataset and flattens the result\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_len + 1))\n",
    "\n",
    "    # Instead of standard scaling all the data, sometimes people\n",
    "    # normalize the window itself wrt to initial element\n",
    "    def normalize_window(w):\n",
    "      return (w/w[0]) -1\n",
    "    if window_scaling:\n",
    "      ds = ds.map(normalize_window)\n",
    "\n",
    "    # randomize order \n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Collect the inputs and the label\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKjdXNaZhwGl"
   },
   "outputs": [],
   "source": [
    "def preprocessing(df, scaler='standard', window_scaling=False, colname='Close', window_len=5, start_date=None, end_date=None, shuffle_buffer=1000,\n",
    "                  batch_size=128, **kwargs):\n",
    "    \"\"\"\n",
    "    Data preprocessing.\n",
    "     - First keep only data between the start and end date\n",
    "     - Compute the log returns from the adjusted close prices\n",
    "     - Use a standard scalar to normalize that data\n",
    "     - Split into training and test sets\n",
    "     \n",
    "     Params:\n",
    "         df - The OHLC dataframe\n",
    "         colname - The column we want to make predictions for (close prices)\n",
    "         window_len - how many elements to use from the series when predicting the next\n",
    "         start_date/end_date - The data range to model over (for example we may want to exclude\n",
    "            the early days of bitcoin with the long tail)\n",
    "        shuffle_buffer - buffer size for shuffling\n",
    "        batch_size - size of batches\n",
    "     \n",
    "     Returns:\n",
    "         training_price_zero - the initial price in the training set (useful when reconstructing prices from rets)\n",
    "         test_price_zero - the initial price in the test set (useful when reconstructing prices from rets)\n",
    "         scaler - the standard scalar (use it to do inverse transform later)\n",
    "         \n",
    "         model_training_data - the windowed dataset and target labels to train the NN on\n",
    "         \n",
    "         training_data - the series of log returns in the training set\n",
    "         test_data - the series of log returns in the test set\n",
    "         \n",
    "         training_dates - the date series for training set\n",
    "         test_dates - the date series for test set\n",
    "    \"\"\"\n",
    "\n",
    "    # Date range of interest\n",
    "    temp_df = df\n",
    "    if start_date is not None:\n",
    "        temp_df = temp_df[temp_df.index >= start_date]\n",
    "    if end_date is not None:\n",
    "        temp_df = temp_df[temp_df.index <= end_date]\n",
    "        \n",
    "    \n",
    "    # First get the log returns\n",
    "    prices_df = temp_df[colname]\n",
    "        \n",
    "    # Split into training/test datasets\n",
    "    training_df, test_df = split_data(prices_df)\n",
    "    \n",
    "    # Want to normalize the log returns (must use same scaler on test and train\n",
    "    # since not supposed to know about test set)\n",
    "    if scaler == 'standard':\n",
    "      print('Standard scaler')\n",
    "      sc = StandardScaler()\n",
    "    elif scaler == 'robust':\n",
    "      sc = RobustScaler()\n",
    "      print('Robust scaler')\n",
    "    elif scaler == 'minmax':\n",
    "      sc = MinMaxScaler()\n",
    "      print('MinMax scaler')\n",
    "    else:\n",
    "      sc = None\n",
    "\n",
    "    if sc is not None:\n",
    "      # Fit on training, transform only the test\n",
    "      training_data = sc.fit_transform(training_df.values.reshape(-1, 1)).flatten()\n",
    "      test_data = sc.transform(test_df.values.reshape(-1, 1)).flatten()\n",
    "      # Remember sc.inverse_transform should transform back the data too so we return the scaler too\n",
    "    else:\n",
    "      print('No scaling')\n",
    "      training_data = training_df.values\n",
    "      test_data = test_df.values\n",
    "      \n",
    "    # Windowed/batched training data to feed the model\n",
    "    windowed_training_data = windowed_dataset(training_data, shuffle_buffer, \n",
    "                                              window_len, batch_size, window_scaling=window_scaling)\n",
    "    \n",
    "    # # This will help with the displaying of results etc\n",
    "    # Training and test dates for plotting comparisons\n",
    "    training_dates = training_df.iloc[window_len: ].index\n",
    "    test_dates = test_df.iloc[window_len:].index\n",
    "\n",
    "    return sc, windowed_training_data, training_data, test_data, training_dates, test_dates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2WbYipwhwGl"
   },
   "outputs": [],
   "source": [
    "def build_model(output_size, neurons, activ_func, dropout, loss, optimizer):\n",
    "  \"\"\"\n",
    "  The keras model. Will try a Conv layer initially followed by a bunch of LSTM layers with dropout.\n",
    "  \n",
    "  Params:\n",
    "     output_size: e.g. predict 1 point in the future\n",
    "     neurons: how many nuerons for each LSTM later\n",
    "     activ_func: activation func, e.g. tanh\n",
    "     loss: loss function to use, e.g. mse\n",
    "     optimizer: e.g. adam\n",
    "     \n",
    "  Returns:\n",
    "     The model\n",
    "  \"\"\"\n",
    "  model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=25, kernel_size=5,\n",
    "                         strides=1, padding=\"causal\",\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=[None, 1]),\n",
    "  # tf.keras.layers.LSTM(neurons, input_shape=[None, 1], return_sequences=True, activation=activ_func),\n",
    "  tf.keras.layers.LSTM(neurons, return_sequences=True, activation=activ_func),\n",
    "  tf.keras.layers.Dropout(dropout),\n",
    "#   tf.keras.layers.LSTM(neurons, return_sequences=True, activation=activ_func),\n",
    "#   tf.keras.layers.Dropout(dropout),\n",
    "#   tf.keras.layers.LSTM(neurons, return_sequences=True, activation=activ_func),\n",
    "#   tf.keras.layers.Dropout(dropout),\n",
    "  tf.keras.layers.LSTM(neurons, return_sequences=False, activation=activ_func),\n",
    "  tf.keras.layers.Dropout(dropout),\n",
    "  tf.keras.layers.Dense(units=output_size, activation='linear'),\n",
    "  ])\n",
    "  model.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoWwjtHdhwGl"
   },
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_len):\n",
    "    \"\"\"\n",
    "    Take the model we just trained and make predictions.\n",
    "    \n",
    "    We window the dataset then try to predict the next values after the window.\n",
    "    Note we do not shuffle this time as we are predicting not training, and want to compare also with\n",
    "    actual prices.\n",
    "    \n",
    "    Parameters:\n",
    "        model: the ML model trained\n",
    "        series: the series on which to make predictions\n",
    "        window_len: size of our window for making preds, e.g. previous 5 elem to predict next perhaps\n",
    "    \"\"\"\n",
    "    # Initially the data is (N,) expand dims to TensorShape([N, 1])\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "\n",
    "    # Now we just use window_len not +1, because we just want inputs not label, and we predict label\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_len, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_len))\n",
    "\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "\n",
    "    return model.predict(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYidh5RPycgF"
   },
   "outputs": [],
   "source": [
    "def denormalize_forecast(forecast, orig_data):\n",
    "    \"\"\"\n",
    "    Convert the predictions back after window normalization\n",
    "\n",
    "    Params:\n",
    "      forecast: our predictions which have been normalized by the first element in \n",
    "                the orig data window\n",
    "      orig_data: the original dataset used to make the predictions\n",
    "    \"\"\"\n",
    "    new_ps = []\n",
    "    for n, p in enumerate(forecast):\n",
    "      w_0 = orig_data[n]\n",
    "      new_p = (p+1) * w_0\n",
    "      new_ps.append(new_p)\n",
    "    return new_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co-Esuy1hwGl"
   },
   "outputs": [],
   "source": [
    "def display_results(model, scaler, dataset, dates, window_len, output_size):\n",
    "    \"\"\"\n",
    "    With our predictions we de-normalize the standardized log returns predicted, then convert them into raw\n",
    "    returns, and finally prices.\n",
    "    \n",
    "    We plot those prices against the actual prices in the same date range\n",
    "    \n",
    "    We compute the MAE and print it.\n",
    "    \n",
    "    Params:\n",
    "        model: the training model\n",
    "        scaler: the scalar we can use to invert the normalization\n",
    "        dataset: Maybe this is train or test set\n",
    "        dates: the dates over this training set for which we expect predictions\n",
    "        window_len: how many previous points used when predicting the next\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "   \n",
    "    preds = model_forecast(model, dataset, window_len)\n",
    "   \n",
    "   \n",
    "    # E.g if window_len is 5, we have predictions for [5:]  since [0, 1, 2, 3, 4] -> [5] etc. If the output_size=1\n",
    "    # then we neglect the final pred since it uses the final 5 elements of training set to pred a subsequent element, which\n",
    "    # we have no training data to compare with\n",
    "    res_df = pd.DataFrame({'y': dataset.flatten()[window_len:], 'yhat': preds.flatten()[:-output_size]})\n",
    "    \n",
    "    # Want to inverse the normalization transform\n",
    "    if scaler is not None:\n",
    "      res_df['y_prices'] = scaler.inverse_transform(res_df['y'].values.reshape(-1, 1)).flatten()\n",
    "      res_df['yhat_prices'] = scaler.inverse_transform(res_df['yhat'].values.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "      # Window scaling\n",
    "      res_df['y_prices'] = res_df['y']\n",
    "      res_df['yhat_prices'] = denormalize_forecast(res_df['yhat'], dataset)\n",
    "                              \n",
    "\n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_scatter(x=dates, y=res_df['y_prices'], mode='lines', name=\"Actual\") \n",
    "\n",
    "    fig.add_scatter(x=dates, y=res_df['yhat_prices'], mode='lines', name=\"Predicted\") \n",
    "\n",
    "    fig.update_layout(template = 'plotly_dark',\n",
    "                      xaxis_title=\"Time\",\n",
    "                      yaxis_title=\"Price\",)\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "                                                  \n",
    "    # Print the MAE                              \n",
    "    mae = mean_absolute_error(res_df['y'], res_df['yhat'])\n",
    "    print(f'The MAE is {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Vs2X4M9hwGl"
   },
   "outputs": [],
   "source": [
    "def display_history(hist):\n",
    "    \"\"\"\n",
    "    Simply plot the training history such as the MAE and Loss over time\n",
    "    \n",
    "    Params:\n",
    "       hist: the history obj returned by Keras\n",
    "       \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig = go.Figure(data=go.Scatter(y=hist.history['loss']))\n",
    "    fig.add_trace(go.Scatter(y=hist.history['mae'],\n",
    "                        mode='lines+markers',\n",
    "                        name='MAE'))\n",
    "    fig.update_layout(template = 'plotly_dark',\n",
    "                      xaxis_title=\"Time\",\n",
    "                      yaxis_title=\"Loss\",)\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsJMX3UShwGm"
   },
   "source": [
    "## Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i9a6Sy1hwGm"
   },
   "outputs": [],
   "source": [
    "## Config dict that defines this attempt\n",
    "config1 = {\n",
    "    \"neurons\": 20,                 # number of hidden units in the LSTM layer\n",
    "    \"activation_function\": 'tanh',   # activation function for LSTM and Dense layer\n",
    "    \"loss\": 'mse',                   # loss function for calculating the gradient, in this case Mean Squared Error. Could be mae\n",
    "    \"optimizer\": 'adam',              # optimizer for appljying gradient decent\n",
    "    \"dropout\": 0.25,                 # dropout ratio used after each LSTM layer to avoid overfitting\n",
    "    \"batch_size\": 128,              \n",
    "    \"epochs\": 40,                  \n",
    "    \"window_len\": 15,                 # is an int to be used as the look back window for creating a single input sample.\n",
    "    \"training_size\": 0.8,            # porportion of data to be used for training\n",
    "    \"shuffle_buffer\": 1000,          # When shuffling the windowed Dataset how many at once to load into memory\n",
    "    \"output_size\": 1,\n",
    "    \"start_date\": None,\n",
    "    \"end_date\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bIklta7mhwGm",
    "outputId": "2db2a08a-0ba2-4813-d7df-25575f6100d4"
   },
   "outputs": [],
   "source": [
    "# Clean up the memory\n",
    "tf.keras.backend.clear_session()\n",
    "btc_model1 = build_model(config1['output_size'], config1['neurons'], config1['activation_function'],\n",
    "                         config1['dropout'], config1['loss'], config1['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7s6BbI43hwGm",
    "outputId": "e01e014f-5809-472e-e8ba-0db7f15d126c"
   },
   "outputs": [],
   "source": [
    "scaler1, model_training_data1, training_data1, test_data1,training_dates1, test_dates1 = preprocessing(int_df, scaler='minmax', **config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eftEqnllhwGm",
    "outputId": "cc8e42df-ee68-4c08-8cec-62ae871ef7d6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btc_history1 = btc_model1.fit(model_training_data1, epochs=config1['epochs'], batch_size=config1['batch_size'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "YFBngkWVhwGn",
    "outputId": "b8291988-6a96-4a54-e1c3-284d4d00fbae"
   },
   "outputs": [],
   "source": [
    "display_history(btc_history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpHwopf1x25v",
    "outputId": "083d200e-ee0d-41d9-96d3-b9fc6ad019ba"
   },
   "outputs": [],
   "source": [
    "btc_model1.save('btcmodel_prices_minmax_1.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "VjWhpqllhwGn",
    "outputId": "ebd21403-2990-4293-ee9c-ec179cb5424a"
   },
   "outputs": [],
   "source": [
    "display_results(btc_model1, scaler1, training_data1, training_dates1, config1['window_len'],\n",
    "                config1['output_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "VDaEWBRCIbJA",
    "outputId": "76fce699-326d-4936-94fa-4e09c06e6ca0"
   },
   "outputs": [],
   "source": [
    "display_results(btc_model1, scaler1, test_data1, test_dates1, config1['window_len'],\n",
    "                config1['output_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-lhXZZOb9v2"
   },
   "source": [
    "# Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's narrow the date window and just train from 2020-09-01 and see if we can get better on the small test set which will be the tail of 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config dict that defines this attempt\n",
    "config2 = {\n",
    "    \"neurons\": 512,                 # number of hidden units in the LSTM layer\n",
    "    \"activation_function\": 'tanh',   # activation function for LSTM and Dense layer\n",
    "    \"loss\": 'mse',                   # loss function for calculating the gradient, in this case Mean Squared Error. Could be mae\n",
    "    \"optimizer\": 'adam',              # optimizer for appljying gradient decent\n",
    "    \"dropout\": 0.25,                 # dropout ratio used after each LSTM layer to avoid overfitting\n",
    "    \"batch_size\": 128,              \n",
    "    \"epochs\": 50,                  \n",
    "    \"window_len\": 15,                 # is an int to be used as the look back window for creating a single input sample.\n",
    "    \"training_size\": 0.8,            # porportion of data to be used for training\n",
    "    \"shuffle_buffer\": 1000,          # When shuffling the windowed Dataset how many at once to load into memory\n",
    "    \"output_size\": 1,\n",
    "    \"start_date\": '2020-09-01',\n",
    "    \"end_date\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the memory\n",
    "tf.keras.backend.clear_session()\n",
    "btc_model2 = build_model(config2['output_size'], config2['neurons'], config2['activation_function'],\n",
    "                         config2['dropout'], config2['loss'], config2['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2, model_training_data2, training_data2, test_data2, training_dates2, test_dates2 = preprocessing(int_df, scaler='minmax', **config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_history2 = btc_model2.fit(model_training_data2, epochs=config2['epochs'], batch_size=config2['batch_size'], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(btc_model2, scaler2, training_data2, training_dates2, config2['window_len'],\n",
    "                config2['output_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(btc_model2, scaler2, test_data2, test_dates2, config2['window_len'],\n",
    "                config2['output_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Hourly-MinMaxPrices-BTC-data.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cap-env",
   "language": "python",
   "name": "cap-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
