{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of re-usable functions. Import it to a given notebook with `%run utils.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be able to use django models etc in the notebook\n",
    "from django_for_jupyter import init_django\n",
    "init_django(project_name='cryptooracle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "#import yfinance as yf\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import scipy.stats as stats\n",
    "import plotly.offline as offline_py\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Stock Django models hold OHLCV data per minute\n",
    "# Sourced from the CSV files here http://www.cryptodatadownload.com/data/gemini/\n",
    "# and then updated with the https://docs.coinapi.io/#latest-data coin API\n",
    "# run ./manage.py import_stocks\n",
    "# and ./manage.py update_stocks to build that database\n",
    "from predictor.models import Stock\n",
    "from django.conf import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Check TF 2.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The database parameters\n",
    "DB_PARAMS = {'host': settings.DATABASES['default']['HOST'],\n",
    "             'port': settings.DATABASES['default']['PORT'],\n",
    "             'database': settings.DATABASES['default']['NAME'],\n",
    "             'user': settings.DATABASES['default']['USER'],\n",
    "             'password': settings.DATABASES['default']['PASSWORD']\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(db_params):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    print(\"Connection successful\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_scheme = {\n",
    "    'index': '#B6B2CF',\n",
    "    'etf': '#2D3ECF',\n",
    "    'tracking_error': '#6F91DE',\n",
    "    'df_header': 'silver',\n",
    "    'df_value': 'white',\n",
    "    'df_line': 'silver',\n",
    "    'heatmap_colorscale': [(0, '#6F91DE'), (0.5, 'grey'), (1, 'red')],\n",
    "    'background_label': '#9dbdd5',\n",
    "    'low_value': '#B6B2CF',\n",
    "    'high_value': '#2D3ECF',\n",
    "    'y_axis_2_text_color': 'grey',\n",
    "    'shadow': 'rgba(0, 0, 0, 0.75)',\n",
    "    'major_line': '#2D3ECF',\n",
    "    'minor_line': '#B6B2CF',\n",
    "    'main_line': 'black'}\n",
    "\n",
    "def generate_config():\n",
    "    return {'showLink': False, 'displayModeBar': False, 'showAxisRangeEntryBoxes': True}\n",
    "\n",
    "def _generate_traces(name_df_color_data):\n",
    "    traces = []\n",
    "\n",
    "    for name, df, color in name_df_color_data:\n",
    "        traces.append(go.Scatter(\n",
    "            name=name,\n",
    "            x=df.index,\n",
    "            y=df,\n",
    "            mode='lines',\n",
    "            line={'color': color}))\n",
    "\n",
    "    return traces\n",
    "\n",
    "def days_to_weeks(open_prices, high_prices, low_prices, close_prices):\n",
    "    \"\"\"Converts daily OHLC prices to weekly OHLC prices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    open_prices : DataFrame\n",
    "        Daily open prices for each ticker and date\n",
    "    high_prices : DataFrame\n",
    "        Daily high prices for each ticker and date\n",
    "    low_prices : DataFrame\n",
    "        Daily low prices for each ticker and date\n",
    "    close_prices : DataFrame\n",
    "        Daily close prices for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    open_prices_weekly : DataFrame\n",
    "        Weekly open prices for each ticker and date\n",
    "    high_prices_weekly : DataFrame\n",
    "        Weekly high prices for each ticker and date\n",
    "    low_prices_weekly : DataFrame\n",
    "        Weekly low prices for each ticker and date\n",
    "    close_prices_weekly : DataFrame\n",
    "        Weekly close prices for each ticker and date\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    open_prices_weekly = open_prices.resample('W').first()\n",
    "    high_prices_weekly = high_prices.resample('W').max()\n",
    "    low_prices_weekly = low_prices.resample('W').min()\n",
    "    close_prices_weekly = close_prices.resample('W').last()\n",
    "    \n",
    "    return open_prices_weekly, high_prices_weekly, low_prices_weekly, close_prices_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df, colname, nbins, with_std_norm=True):\n",
    "\n",
    "    fig = px.histogram(df[colname] , x=colname, marginal=\"violin\",\n",
    "                       hover_data=[colname], histnorm='probability', nbins=nbins)\n",
    "\n",
    "    if with_std_norm:\n",
    "        # Standard normal\n",
    "        ndist = stats.norm(loc=0, scale=1)\n",
    "        low_bound = ndist.ppf(.0001)\n",
    "        high_bound = ndist.ppf(.999999)\n",
    "        x_norm = np.linspace(low_bound, high_bound, 201)\n",
    "        y_norm = ndist.pdf(x_norm)\n",
    "\n",
    "        fig.add_traces(go.Scatter(x=x_norm, y=y_norm, mode = 'lines',\n",
    "                                  line = dict(color='rgba(0,255,0, 0.6)',\n",
    "                                              width = 1\n",
    "                                             ),\n",
    "                                  name = 'normal'\n",
    "                                 ))\n",
    "\n",
    "    fig.update_layout(template = 'plotly_dark')\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot(series, title, xlabel, ylabel, plot_type='scatter'):\n",
    "    #pio.renderers.default = 'colab'\n",
    "    if plot_type == 'scatter':\n",
    "        fig = px.scatter(x=series.index, y=series.values)\n",
    "    elif plot_type == 'bar':\n",
    "        fig = px.bar(x=series.index, y=series.values, width=1000 * 3600 * 24 * 1)\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=xlabel,\n",
    "        yaxis_title=ylabel,\n",
    "        font=dict(\n",
    "            family=\"Courier New, monospace\",\n",
    "            size=18,\n",
    "            color=\"RebeccaPurple\"\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stocks_df():\n",
    "    conn = connect(DB_PARAMS)\n",
    "    df = pd.read_sql_query('select * from \"predictor_stock\"', con=conn)\n",
    "\n",
    "    # Cleanup\n",
    "    df.drop(columns=['id', 'name', 'updated', 'created', 'open', 'high', 'low'], inplace=True)\n",
    "    df.set_index('dt', inplace=True)\n",
    "    # Timezone aware date\n",
    "    df.index = pd.to_datetime(df.index, utc=True)\n",
    "    # Ensure sorted\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(prices):\n",
    "    \"\"\"\n",
    "    Compute returns from prices (adj close prices for example)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_returns : DataFrame\n",
    "        Log returns for each ticker and date\n",
    "    \"\"\"\n",
    "    return (prices- prices.shift(1))/prices.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_returns(prices):\n",
    "    \"\"\"\n",
    "    Compute log returns \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_returns : DataFrame\n",
    "        Log returns for each ticker and date\n",
    "    \"\"\"\n",
    "    return np.log(prices) - np.log(prices.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_returns_to_returns(log_returns):\n",
    "    \"\"\"\n",
    "    Convert log returns back into returns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_returns : Series\n",
    "        Log returns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    returns : The raw returns\n",
    "    \"\"\"\n",
    "    return np.exp(log_returns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, colname):\n",
    "    \"\"\"\n",
    "    Standard by subtracting the mean and diving by the standard deviation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    colnamne: column name to apply standardization to    -------\n",
    "    log_returns : DataFrame\n",
    "        Log returns for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "\n",
    "    \"\"\"\n",
    "    mean = df[colname].mean()\n",
    "    std = df[colname].std()\n",
    "    return (df[colname]-mean)/(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstandardize(df, colname, mean, std):\n",
    "    \"\"\"\n",
    "    Unstandardize by multiplyging by the std and adding the mean of the original distr\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    colnamne: column name to apply standardization to    -------\n",
    "    mean: mean of orig distribution\n",
    "    std: std dev of orig distribution\n",
    "    \n",
    "    Returns\n",
    "        unstandardized series\n",
    "    \"\"\"\n",
    "    return (std*df[colname])+mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_to_prices(df, colname, price_zero):\n",
    "    \"\"\"\n",
    "    Starting with price at time 0, `p_0`, then get the series of prices\n",
    "    e.g p_n = (1_r_n)...(1+r1)p_0\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "    colnamne: returns column name to compute prices from\n",
    "    price_zero: starting price\n",
    "    \n",
    "    Returns\n",
    "        series of prices\n",
    "    \"\"\"\n",
    "    return price_zero*df[colname].fillna(0).add(1).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, training_size=0.8):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets\n",
    "    We want to preserve order and not shuffle at this stage as past points will be used to predict next in the sequence\n",
    "    The test set will represent the unseen future\n",
    "    However the windowed dataset will shuffle around each windows and along with target label (see that func)\n",
    "    \n",
    "    Params:\n",
    "        data: the dataset\n",
    "        training_sie: the split e.g. 0.8 means 80% of data is used in the training set\n",
    "    \"\"\"\n",
    "    return data[:int(training_size*len(data))], data[int(training_size*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, shuffle_buffer, window_len, batch_size, window_scaling=False, multivar=False):\n",
    "    \"\"\"\n",
    "    If we have a series like [1,2,3,4,5,6]\n",
    "    We want to split it into windows, e.g. we take previous value of the series\n",
    "    as the X input features and want to predict the following value as output Y\n",
    "    e.g. [1,2] - > 3, so what we want to do is split the data into windows\n",
    "    of length window_len + 1 (the +1 acconts for the label)\n",
    "    E.g.\n",
    "      [1, 2, 3]\n",
    "      [2, 3, 4]\n",
    "      [3, 4, 5]\n",
    "      [4, 5, 6]\n",
    "\n",
    "\n",
    "    We shuffle the data to avoid bias\n",
    "\n",
    "    Finally we split the example into input/target\n",
    "\n",
    "    [[1, 2], [3]]\n",
    "    .\n",
    "    .\n",
    "\n",
    "    so it's appropriate to feed into the model.fit    \n",
    "\n",
    "    and we batch it into batches of batch_size\n",
    "    \n",
    "    Params:\n",
    "        series: the series upon which we perform the windowing\n",
    "        shuffle_buffer: size of the buffer when shuffling\n",
    "            (see https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#shuffle)\n",
    "        window_len; how many previous elements to take into account when predicting the next\n",
    "        batch_size: https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#batch\n",
    "    \"\"\"\n",
    "    # Initially the data is (1188,) expand dims to TensorShape([1188, 1])\n",
    "    if not multivar:\n",
    "        series = tf.expand_dims(series, axis=-1)\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "    # will be an iterable of tf.Tensor([998.325], shape=(1,), dtype=float32),...\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/55429307/how-to-use-windows-created-by-the-dataset-window-method-in-tensorflow-2-0\n",
    "    # The +1 accounts for the label too. Create a bunch of windows over our series\n",
    "    # If we started with ds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5])\n",
    "    # then ds = ds.window(3, shift=1, drop_remainder=False) would lead\n",
    "    # to [1,2,3], [2, 3, 4], [3, 4, 5], [4, 5], [5] whereas \n",
    "    # drop_remainder=True) => [1,2,3], [2, 3, 4], [3, 4, 5]\n",
    "    # Remember the first window_len are our training data and the 1 is \n",
    "    # the target/label\n",
    "    # Could also do this with pandas shift\n",
    "    ds = ds.window(window_len + 1, shift=1, drop_remainder=True)\n",
    "    #for w in ds:\n",
    "    #    print(list(w.as_numpy_iterator()))\n",
    "    \n",
    "    # Maps map_func across this dataset and flattens the result\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_len + 1))\n",
    "\n",
    "    # Instead of standard scaling all the data, sometimes people\n",
    "    # normalize the window itself wrt to initial element\n",
    "    def normalize_window(w):\n",
    "      return (w/w[0]) -1\n",
    "    if window_scaling:\n",
    "      ds = ds.map(normalize_window)\n",
    "\n",
    "    # randomize order \n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Collect the inputs and the label\n",
    "    if multivar:\n",
    "        # All rows except last (all feats), but then just the first col of last row (target output close)\n",
    "        ds = ds.map(lambda w: (w[:-1, :], w[-1, 0]))\n",
    "    else:\n",
    "        ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(output_size, neurons, activ_func, dropout, loss, optimizer, input_shape=1):\n",
    "  \"\"\"\n",
    "  The keras model. Will try a Conv layer initially followed by a bunch of LSTM layers with dropout.\n",
    "  \n",
    "  Params:\n",
    "     output_size: e.g. predict 1 point in the future\n",
    "     neurons: how many nuerons for each LSTM later\n",
    "     activ_func: activation func, e.g. tanh\n",
    "     loss: loss function to use, e.g. mse\n",
    "     optimizer: e.g. adam\n",
    "     \n",
    "  Returns:\n",
    "     The model\n",
    "  \"\"\"\n",
    "  model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv1D(filters=25, kernel_size=5,\n",
    "                         strides=1, padding=\"causal\",\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=[None, input_shape]),\n",
    "  # tf.keras.layers.LSTM(neurons, input_shape=[None, None, 1], return_sequences=True, activation=activ_func),\n",
    "  tf.keras.layers.LSTM(neurons, return_sequences=True, activation=activ_func),\n",
    "  tf.keras.layers.Dropout(dropout),\n",
    "#   tf.keras.layers.LSTM(neurons, return_sequences=True, activation=activ_func),\n",
    "#   tf.keras.layers.Dropout(dropout),\n",
    "#   tf.keras.layers.LSTM(neurons, return_sequences=True, activation=activ_func),\n",
    "#   tf.keras.layers.Dropout(dropout),\n",
    "      \n",
    "  # Ensure last layer has return sequences Flae\n",
    "  tf.keras.layers.LSTM(neurons, return_sequences=False, activation=activ_func),\n",
    "  tf.keras.layers.Dropout(dropout),\n",
    "  tf.keras.layers.Dense(units=output_size, activation=activ_func),\n",
    "  ])\n",
    "  model.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(model, scaler, dataset, dates, window_len, output_size, multivar=False):\n",
    "    \"\"\"\n",
    "    With our predictions we de-normalize the predictions using the inverse transform.\n",
    "    \n",
    "    We plot those prices against the actual prices in the same date range\n",
    "    \n",
    "    We compute the MAE and print it.\n",
    "    \n",
    "    Params:\n",
    "        model: the training model\n",
    "        scaler: the scalar we can use to invert the normalization\n",
    "        dataset: Maybe this is train or test set\n",
    "        dates: the dates over this set for which we expect predictions\n",
    "        window_len: how many previous points used when predicting the next\n",
    "        output_size: how many steps forward are we predicting\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "   \n",
    "    preds = model_forecast(model, dataset, window_len, multivar=multivar)\n",
    "   \n",
    "   \n",
    "    # E.g if window_len is 5, we have predictions for [5:]  since [0, 1, 2, 3, 4] -> [5] etc. If the output_size=1\n",
    "    # then we neglect the final pred since it uses the final 5 elements of training set to pred a subsequent element, which\n",
    "    # we have no training data to compare with\n",
    "    if not multivar:\n",
    "        res_df = pd.DataFrame({'y': dataset.flatten()[window_len:], 'yhat': preds.flatten()[:-output_size]})\n",
    "    else:\n",
    "        res_df = pd.DataFrame({'y': dataset[window_len:, 0], 'yhat': preds[:-output_size, 0]})\n",
    "    \n",
    "    # Want to inverse the normalization transform\n",
    "    if scaler is not None:\n",
    "        if not multivar:\n",
    "            res_df['y_prices'] = scaler.inverse_transform(res_df['y'].values.reshape(-1, 1)).flatten()\n",
    "            res_df['yhat_prices'] = scaler.inverse_transform(res_df['yhat'].values.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            # Make a fake col to satisfy the scaler which was trained with close and vol\n",
    "            dummy_col = np.random.random((res_df['y'].shape[0], 1))\n",
    "            expanded_y = np.hstack((res_df['y'].values.reshape(-1, 1), dummy_col))\n",
    "            expanded_yhat = np.hstack((res_df['yhat'].values.reshape(-1, 1), dummy_col))\n",
    "            res_df['y_prices'] = scaler.inverse_transform(expanded_y)[:, 0]\n",
    "            res_df['yhat_prices'] = scaler.inverse_transform(expanded_yhat)[:, 0]\n",
    "    else:\n",
    "      # Window scaling\n",
    "      res_df['y_prices'] = res_df['y']\n",
    "      res_df['yhat_prices'] = denormalize_forecast(res_df['yhat'], dataset)\n",
    "                              \n",
    "\n",
    "    # Plot\n",
    "    fig = go.Figure()\n",
    "    fig.add_scatter(x=dates, y=res_df['y_prices'], mode='lines', name=\"Actual\") \n",
    "\n",
    "    fig.add_scatter(x=dates, y=res_df['yhat_prices'], mode='lines', name=\"Predicted\") \n",
    "\n",
    "    fig.update_layout(template = 'plotly_dark',\n",
    "                      xaxis_title=\"Time\",\n",
    "                      yaxis_title=\"Price\",)\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "                                                  \n",
    "    # Print the MAE                              \n",
    "    mae = mean_absolute_error(res_df['y'], res_df['yhat'])\n",
    "    print(f'The MAE is {mae}')\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_history(hist):\n",
    "    \"\"\"\n",
    "    Simply plot the training history such as the MAE and Loss over time\n",
    "    \n",
    "    Params:\n",
    "       hist: the history obj returned by Keras\n",
    "       \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig = px.line(hist.history['loss'], title='Loss over time')\n",
    "\n",
    "    fig.update_layout(template = 'plotly_dark',\n",
    "                      xaxis_title=\"Time\",\n",
    "                      yaxis_title=\"Loss\",)\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    fig = px.line(hist.history['mae'], title='MAE over time')\n",
    "\n",
    "    fig.update_layout(template = 'plotly_dark',\n",
    "                      xaxis_title=\"Time\",\n",
    "                      yaxis_title=\"MAE\",)\n",
    "\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, scaler='standard', window_scaling=False, colname='close', window_len=5, start_date=None,\n",
    "                  end_date=None, shuffle_buffer=1000,\n",
    "                  batch_size=128, **kwargs):\n",
    "    \"\"\"\n",
    "    Data preprocessing.\n",
    "     - First keep only data between the start and end date\n",
    "     - Compute the log returns from the adjusted close prices\n",
    "     - Use a standard scalar to normalize that data\n",
    "     - Split into training and test sets\n",
    "     \n",
    "     Params:\n",
    "         df - The OHLC dataframe\n",
    "         colname - The column we want to make predictions for (close prices)\n",
    "         window_len - how many elements to use from the series when predicting the next\n",
    "         start_date/end_date - The data range to model over (for example we may want to exclude\n",
    "            the early days of bitcoin with the long tail)\n",
    "        shuffle_buffer - buffer size for shuffling\n",
    "        batch_size - size of batches\n",
    "     \n",
    "     Returns:\n",
    "         training_price_zero - the initial price in the training set (useful when reconstructing prices from rets)\n",
    "         test_price_zero - the initial price in the test set (useful when reconstructing prices from rets)\n",
    "         scaler - the standard scalar (use it to do inverse transform later)\n",
    "         \n",
    "         model_training_data - the windowed dataset and target labels to train the NN on\n",
    "         \n",
    "         training_data - the series of log returns in the training set\n",
    "         test_data - the series of log returns in the test set\n",
    "         \n",
    "         training_dates - the date series for training set\n",
    "         test_dates - the date series for test set\n",
    "    \"\"\"\n",
    "\n",
    "    # Date range of interest\n",
    "    temp_df = df\n",
    "    if start_date is not None:\n",
    "        temp_df = temp_df[temp_df.index >= start_date]\n",
    "    if end_date is not None:\n",
    "        temp_df = temp_df[temp_df.index <= end_date]\n",
    "        \n",
    "    \n",
    "    # First get the log returns\n",
    "    prices_df = temp_df[colname]\n",
    "        \n",
    "    # Split into training/test datasets\n",
    "    training_df, test_df = split_data(prices_df)\n",
    "    \n",
    "    # Want to normalize the log returns (must use same scaler on test and train\n",
    "    # since not supposed to know about test set)\n",
    "    if scaler == 'standard':\n",
    "      print('Standard scaler')\n",
    "      sc = StandardScaler()\n",
    "    elif scaler == 'robust':\n",
    "      sc = RobustScaler()\n",
    "      print('Robust scaler')\n",
    "    elif scaler == 'minmax':\n",
    "      sc = MinMaxScaler()\n",
    "      print('MinMax scaler')\n",
    "    else:\n",
    "      sc = None\n",
    "\n",
    "    if sc is not None:\n",
    "      # Fit on training, transform only the test\n",
    "      training_data = sc.fit_transform(training_df.values.reshape(-1, 1)).flatten()\n",
    "      test_data = sc.transform(test_df.values.reshape(-1, 1)).flatten()\n",
    "      # Remember sc.inverse_transform should transform back the data too so we return the scaler too\n",
    "    else:\n",
    "      print('No scaling')\n",
    "      training_data = training_df.values\n",
    "      test_data = test_df.values\n",
    "      \n",
    "    # Windowed/batched training data to feed the model\n",
    "    windowed_training_data = windowed_dataset(training_data, shuffle_buffer, \n",
    "                                              window_len, batch_size, window_scaling=window_scaling)\n",
    "    \n",
    "    # # This will help with the displaying of results etc\n",
    "    # Training and test dates for plotting comparisons\n",
    "    training_dates = training_df.iloc[window_len: ].index\n",
    "    test_dates = test_df.iloc[window_len:].index\n",
    "\n",
    "    return sc, windowed_training_data, training_data, test_data, training_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forecast(model, series, window_len, multivar=False):\n",
    "    \"\"\"\n",
    "    Take the model we just trained and make predictions.\n",
    "    \n",
    "    We window the dataset then try to predict the next values after the window.\n",
    "    Note we do not shuffle this time as we are predicting not training, and want to compare also with\n",
    "    actual prices.\n",
    "    \n",
    "    Parameters:\n",
    "        model: the ML model trained\n",
    "        series: the series on which to make predictions\n",
    "        window_len: size of our window for making preds, e.g. previous 5 elem to predict next perhaps\n",
    "    \"\"\"\n",
    "    if not multivar:\n",
    "        # Initially the data is (N,) expand dims to TensorShape([N, 1])\n",
    "        series = tf.expand_dims(series, axis=-1)\n",
    "\n",
    "    # Now we just use window_len not +1, because we just want inputs not label, and we predict label\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_len, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_len))\n",
    "\n",
    "    ds = ds.batch(32).prefetch(1)\n",
    "\n",
    "    return model.predict(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivar_preprocessing(df, scaler='standard', window_scaling=False, colnames=['close', 'volume'], window_len=5, start_date=None,\n",
    "                           end_date=None, shuffle_buffer=1000,\n",
    "                           batch_size=128, **kwargs):\n",
    "    \"\"\"\n",
    "    Data preprocessing.\n",
    "     - First keep only data between the start and end date\n",
    "     - Compute the log returns from the adjusted close prices\n",
    "     - Use a standard scalar to normalize that data\n",
    "     - Split into training and test sets\n",
    "     \n",
    "     Params:\n",
    "         df - The OHLC dataframe\n",
    "         colnames - The columns we want to make predictions from (volume, close prices)\n",
    "         window_len - how many elements to use from the series when predicting the next\n",
    "         start_date/end_date - The data range to model over (for example we may want to exclude\n",
    "            the early days of bitcoin with the long tail)\n",
    "        shuffle_buffer - buffer size for shuffling\n",
    "        batch_size - size of batches\n",
    "     \n",
    "     Returns:\n",
    "         training_price_zero - the initial price in the training set (useful when reconstructing prices from rets)\n",
    "         test_price_zero - the initial price in the test set (useful when reconstructing prices from rets)\n",
    "         sc1 - the standard scalar (use it to do inverse transform later)\n",
    "         sc2 - scaler for th\n",
    "         \n",
    "         model_training_data - the windowed dataset and target labels to train the NN on\n",
    "         \n",
    "         training_data - the series of log returns in the training set\n",
    "         test_data - the series of log returns in the test set\n",
    "         \n",
    "         training_dates - the date series for training set\n",
    "         test_dates - the date series for test set\n",
    "    \"\"\"\n",
    "\n",
    "    # Date range of interest\n",
    "    temp_df = df\n",
    "    if start_date is not None:\n",
    "        temp_df = temp_df[temp_df.index >= start_date]\n",
    "    if end_date is not None:\n",
    "        temp_df = temp_df[temp_df.index <= end_date]\n",
    "        \n",
    "    \n",
    "    # First get the log returns\n",
    "    prices_df = temp_df[colnames]\n",
    "        \n",
    "    # Split into training/test datasets\n",
    "    training_df, test_df = split_data(prices_df)\n",
    "    \n",
    "    # Want to normalize the log returns (must use same scaler on test and train\n",
    "    # since not supposed to know about test set)\n",
    "    if scaler == 'standard':\n",
    "      print('Standard scaler')\n",
    "      sc = StandardScaler()\n",
    "    elif scaler == 'robust':\n",
    "      sc = RobustScaler()\n",
    "      print('Robust scaler')\n",
    "    elif scaler == 'minmax':\n",
    "      sc = MinMaxScaler()\n",
    "      print('MinMax scaler')\n",
    "    else:\n",
    "      sc = None\n",
    "\n",
    "    if sc is not None:\n",
    "      # Fit on training, transform only the test\n",
    "      training_data = sc.fit_transform(training_df.values)\n",
    "      test_data = sc.transform(test_df)\n",
    "      # Remember sc.inverse_transform should transform back the data too so we return the scaler too\n",
    "    else:\n",
    "      print('No scaling')\n",
    "      training_data = training_df.values\n",
    "      test_data = test_df.values\n",
    "      \n",
    "    # Windowed/batched training data to feed the model\n",
    "    windowed_training_data = windowed_dataset(training_data, shuffle_buffer, \n",
    "                                              window_len, batch_size, window_scaling=window_scaling, multivar=True)\n",
    "    \n",
    "    # # This will help with the displaying of results etc\n",
    "    # Training and test dates for plotting comparisons\n",
    "    training_dates = training_df.iloc[window_len: ].index\n",
    "    test_dates = test_df.iloc[window_len:].index\n",
    "\n",
    "    return sc, windowed_training_data, training_data, test_data, training_dates, test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cypto-oracle-venv",
   "language": "python",
   "name": "cypt-oracle-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
